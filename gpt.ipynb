{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 256\n",
    "n_embed = 384\n",
    "num_heads = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "eval_interval = 500\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ''\n",
    "with open('data/input.txt', 'r') as f:\n",
    "    input = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all characters used in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(list(set(input)))\n",
    "vocab_size = len(characters)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encode and decode functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(str):\n",
    "    encoded = []\n",
    "    for c in str:\n",
    "        encoded.append(characters.index(c))\n",
    "    return encoded\n",
    "\n",
    "def decode(codes):\n",
    "    decoded = ''\n",
    "    for code in codes:\n",
    "        decoded = decoded + characters[code]\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode data and separate it into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(input), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "training_data = data[:n]\n",
    "validation_data = data[n:]\n",
    "print(len(training_data), len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size, block size, and get_batch function. The batch and block size help send data to the GPU in batches for more efficient training. The targets batches are offset by 1 from the inputs batches, because we will be passing the inputs to the transformer, and the targets should be the predicted output given those inputs, hense training the transformer how to predict the next sequece of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 26, 53,  ..., 56, 43, 47],\n",
      "        [60, 43, 56,  ..., 56,  1, 41],\n",
      "        [26, 21, 33,  ..., 26, 21, 13],\n",
      "        ...,\n",
      "        [ 5, 57,  1,  ...,  1, 35, 47],\n",
      "        [56, 53, 53,  ..., 59, 50, 42],\n",
      "        [42, 47, 56,  ..., 39, 56,  1]], device='cuda:0')\n",
      "tensor([[26, 53, 58,  ..., 43, 47, 45],\n",
      "        [43, 56,  1,  ...,  1, 41, 53],\n",
      "        [21, 33, 31,  ..., 21, 13, 10],\n",
      "        ...,\n",
      "        [57,  1, 52,  ..., 35, 47, 50],\n",
      "        [53, 53, 58,  ..., 50, 42,  1],\n",
      "        [47, 56, 43,  ..., 56,  1, 51]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data):\n",
    "    random_offsets = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    inputs = torch.stack([data[i:i+block_size] for i in random_offsets])\n",
    "    targets = torch.stack([data[i+1:i+block_size+1] for i in random_offsets])\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = get_batch(training_data)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer block\n",
    "    def __init__(self, n_embed, n_head) -> None:\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiAttentionHead(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ln1x = self.ln1(x)\n",
    "        ln2x = self.ln2(x)\n",
    "        x = x + self.sa(ln1x)\n",
    "        x = x + self.ffwd(ln2x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1D:\n",
    "    # Layer Norm\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 65])\n",
      "tensor(4.3389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "iIYcfZdn,jYB&,!WEwD!.McsSN.cEpY?FCmvQAbnXkWHvVw!?FlPJKiNuJkUOByuyuayo:E'!:q?UIQVdWPARgnngXeotAT\n",
      "3hxv\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, num_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, C\n",
    "        x = tok_emb + pos_emb # B,T,C\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # B,T,vocab_size\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)    \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B,T\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:,-1,:] # becomes (B,C)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "logits, loss = m(inputs, targets)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated = m.generate(idx=context, max_new_tokens=100)[0].tolist()\n",
    "print(decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for i, split in enumerate([training_data, validation_data]):\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            _, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[i] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train 1oss 4.3359, val loss 4.3320\n",
      "step 500: train 1oss 2.0385, val loss 2.1175\n",
      "step 1000: train 1oss 1.6418, val loss 1.8124\n",
      "step 1500: train 1oss 1.4722, val loss 1.6656\n",
      "step 2000: train 1oss 1.3713, val loss 1.5915\n",
      "step 2500: train 1oss 1.3105, val loss 1.5516\n",
      "step 3000: train 1oss 1.2568, val loss 1.5221\n",
      "step 3500: train 1oss 1.2151, val loss 1.5037\n",
      "step 4000: train 1oss 1.1785, val loss 1.4897\n",
      "step 4500: train 1oss 1.1454, val loss 1.4839\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        training_losses = losses[0]\n",
    "        validation_losses = losses[1]\n",
    "        print(f\"step {steps}: train 1oss {training_losses:.4f}, val loss {validation_losses:.4f}\")\n",
    "\n",
    "    inputs, targets = get_batch(training_data)\n",
    "\n",
    "    logits, loss = m(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LADY GREY:\n",
      "Holishour.\n",
      "\n",
      "GLOUCEST:\n",
      "And me, whither doth this day loving does?\n",
      "\n",
      "PERDITA:\n",
      "O her!\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, good stand I forbear?\n",
      "\n",
      "GLOUCESTER:\n",
      "Why ohopen early prost thou to act morrow;\n",
      "Live on thy prince and now too rison will,\n",
      "By vault foresh were needly store fulling sorrow.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Welcome, see his brother, who sun begin,\n",
      "His entreaten in thy blood, as on his tongues\n",
      "Which blind to vey'll make his good tie mages.\n",
      "Go chost the wrinks so groan;\n",
      "And let for what is a grance to cheal\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated = m.generate(idx=context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "torch.save(model.state_dict(), 'transformer_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CORIOLANUS:\n",
      "I have no suit fair?\n",
      "Sister'd more beautifieds; for you have piny,\n",
      "Which you pany of this temper'd with you.\n",
      "\n",
      "Second Servingman:\n",
      "We will, so you shall be honour; who has at you\n",
      "Hunts you would Gremio, have stoned and Gaunt\n",
      "A true deeds to give your sorrow;\n",
      "You are you but confessor, sir.\n",
      "\n",
      "KING LEWIS XI:\n",
      "Go, you'll be gone, for I pardon my soul oath.\n",
      "\n",
      "ANGELO:\n",
      "Good my leave was to me.\n",
      "\n",
      "LADY GREY:\n",
      "Good night, I think, good my lord, I make constant no pair;\n",
      "For I kneel a word in England's claim'd\n",
      "Girl have brock'd from the friendship safety.\n",
      "I dread, proud heaven! O my son\n",
      "You desire have of my son! I am at thy oath\n",
      "That like us fancy is as God, to pass yourself accasion.\n",
      "Welcome, sir, come fear, three flood with us\n",
      "his gracious parts, to queen, that vawards from his wife:\n",
      "For my both to chance it well. What should you not your name?\n",
      "I know not you, lords, sweet Clarence: dry the queen,\n",
      "When you do forswear this noble of absence;\n",
      "For blind the Coventry, now God the child;\n",
      "He cannot did be gone age, and bear noble lords.\n",
      "\n",
      "BONA:\n",
      "It is your command; and I come.\n",
      "My scepted at shame your leave or street,\n",
      "Or horses you like a request, and to a hand\n",
      "For Oxford in this whore he sights and of fair:\n",
      "But swill I wrong'd peace; I will not strew your pleasure\n",
      "There access bills that The common will escend;\n",
      "And why my counterfeit should not you be a poor.\n",
      "\n",
      "JULIET:\n",
      "O, rup his natural, which you be as you all!\n",
      "\n",
      "JULIET:\n",
      "Wilt not haught you shall here be and his state?\n",
      "With Juliet of honour answer like a battle,\n",
      "His worthy soldier Volsce, winters than you,\n",
      "But by these cools, and with a rise from him.\n",
      "3 KING HENRY VI\n",
      "\n",
      "KING HENRY VI:\n",
      "Farewell, I shall long to home with you.\n",
      "\n",
      "KING HENRY VI:\n",
      "Very Warwick both the King RICHARD II:\n",
      "Had the exilence, I was a nobleness blush.\n",
      "Sweet Northumberland, no, look, dost thou all:\n",
      "As shall be the moon of his sudject,\n",
      "Make she made her redeed in small early see,\n",
      "And company the civil of Claudio's death,\n",
      "But loss'd with Bolingbroke thousand his words.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Well, you disgraced him to cheque,\n",
      "So with the crown and me pay of an our dream?\n",
      "Having enough to make a sorrow most beauty;\n",
      "But, as I have you pan thee, whose pale an ear?\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Good forbid! It is my father's fortune's head; there's\n",
      "Live such sweet in his dear 'joy,' Escalus' please:'\n",
      "His prayer Henry and York!'\n",
      "The traitor maids of heavy left bore his head.\n",
      "Methinks the palasure of his own again:\n",
      "O, Braves; youngers and keep him what so say you,\n",
      "If you are, his past of leave, the black again.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "But you'll answer out to him.\n",
      "\n",
      "KING RICHARD III:\n",
      "I hope you hearness a great lord.\n",
      "\n",
      "KING RICHARD III:\n",
      "If it can, let me go.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, so, well, besides you look'd your knackerness?\n",
      "\n",
      "KING HENRY VI:\n",
      "That sed so, I cannot straight you.\n",
      "\n",
      "KING RICHARD III:\n",
      "What, what, gave prevail'd for the crown?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Come to their children. Come, come with their fools?\n",
      "Survy death, and their kindness as seen,\n",
      "But for revoke, as soon bright corns up the country!\n",
      "\n",
      "JULIET:\n",
      "Let me better him chosen in mine honour;\n",
      "And when he has lid me well be glad:\n",
      "And by complain, I would not account you:\n",
      "Once more victory by great Warwick comes!\n",
      "\n",
      "GLOUCESTER:\n",
      "How comest me a father for a blow man!\n",
      "The larks, thereI was clotched, wouldst that farewell\n",
      "But but of the unsague to desperate for you for\n",
      "A sister's country of blood.\n",
      "\n",
      "GLOUCESTER:\n",
      "Repent!\n",
      "Why stand why, is we here? no, no, no; our place.\n",
      "\n",
      "GLOUCESTER:\n",
      "Well have a gentleman of the neck\n",
      "Until the maid, and time the viperts of all pass; and,\n",
      "And who early man, that is dead, whose duke it me,\n",
      "Even look'd most passage, who will perish all vantage about.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Madam be, you have found a plotted to a guard,\n",
      "Whereupon the stretched may pity canst\n",
      "you di know thee daughterer, thou hast compass for\n",
      "The mother of late?\n",
      "You may coming you hither with you do command.\n",
      "\n",
      "QUEEN:\n",
      "Ay, sir, if you King Henry poison,\n",
      "The cry'st justice of good; our battle, I have seen a\n",
      "lark, and all it now full of a king.\n",
      "\n",
      "QUEEN:\n",
      "Here's ABHORSON:\n",
      "No more, no, I'll have present you:\n",
      "The first to your leisure,--\n",
      "\n",
      "QUEEN:\n",
      "Why should have strange to beg, but not thy life,\n",
      "To put upon'd him; he is a best of thy body based\n",
      "Any hearts I proper should not power to pluck\n",
      "One date his rogue, now to the woe.\n",
      "The mother dog for groans.\n",
      "\n",
      "KING RICHARD III:\n",
      "Here comes, madam; what married up your veins\n",
      "Have most exroclaims on Half or Apollo,\n",
      "That like it may like an air Perhap Tybalt.\n",
      "\n",
      "KING RICHARD II:\n",
      "Lord Angelo's mack, and my knee I seal in form.\n",
      "\n",
      "KING RICHARD II:\n",
      "I know you hear me live, and by Clarence,\n",
      "Hath compounded in his yielding head:\n",
      "You have for King Lewis forces dead.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why, I say, trust thou weep'st, Warwick with womb!\n",
      "\n",
      "QUEEN:\n",
      "Garden, madam, I love, I pardon my part.\n",
      "\n",
      "KING LEWIS XI:\n",
      "Why, is a legs?\n",
      "\n",
      "KING HENRY VI:\n",
      "Father, gentle marriage; who love me hither?\n",
      "\n",
      "RIVERS:\n",
      "The shame, is a quarrel; and weak a disposition,\n",
      "And his left in his rash, and so I may must\n",
      "To e\n"
     ]
    }
   ],
   "source": [
    "test_load_model = BigramLanguageModel()\n",
    "tm = test_load_model.to(device)\n",
    "tm.load_state_dict(torch.load('shakespear_transformer_params.pth'))\n",
    "tm.eval()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated = tm.generate(idx=context, max_new_tokens=5000)[0].tolist()\n",
    "print(decode(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
